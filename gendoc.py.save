import os, sys
import glob
import argparse
import numpy as np
import pandas as pd
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
import string
import re

# gendoc.py -- Don't forget to put a reasonable amount code comments
# in so that we better understand what you're doing when we grade!

# add whatever additional imports you may need here

parser = argparse.ArgumentParser(description="Generate term-document matrix.")
parser.add_argument("-T", "--tfidf", action="store_true", help="Apply tf-idf to the matrix.")
parser.add_argument("-S", "--svd", metavar="N", dest="svddims", type=int,
                    default=None,
                    help="Use TruncatedSVD to truncate to N dimensions")
parser.add_argument("-B", "--base-vocab", metavar="M", dest="basedims",
                    type=int, default=None,
                    help="Use the top M dims from the raw counts before further processing")
parser.add_argument("foldername", type=str,
                    help="The base folder name containing the two topic subfolders.")
parser.add_argument("outputfile", type=str,
                    help="The name of the output file for the matrix data.")

args = parser.parse_args()
m = args.basedims

if args.basedims:
	cv = CountVectorizer(max_features=m)
else:
     	cv = CountVectorizer()

# base_foldername = os.listdir(args.foldername)
# # join list (args.foldername, subfolder)
# for doc in base_foldername:
# read.doc.lower()
# # df = pf.DataFrame(words or counts)

# read in aaaaaall the words

def fetchtokens(foldername): # this would be scratch
    '''Read all words
    '''

    # df = pd.DataFrame(counts)
    '''corpus:
        array containing all the documents
    '''
    corpus = []
    counts = {}
    data = {}

    remove_punct = str.maketrans('', '', string.punctuation)

    # maybe to get labels? labels = glob.glob(article***.txt)

    base_folders = os.listdir(args.foldername) #list containing grain + crude
    for folder in base_folders: #[grain,crude]
        subfolder_path = os.path.join(args.foldername, folder) #grain,crude, 
        for txtfile in os.listdir(subfolder_path):
            file_path = os.path.join(subfolder_path, txtfile) #os.listdir(crude)
            '''
            For each document, remove punctuation and create word vectors
            '''
            with open(file_path, 'r') as f:
                '''read in the entire file'''
                docstring = f.read().replace('\n', '')
                for line in f:
                    line = line.translate(remove_punct).lower().split()
            corpus.append(docstring)
    return corpus

# tv = TfidfVectorizer()

corpus = fetchtokens(args.foldername)

if args.T:
       	x = cv.fit_transform(corpus)
else:
	x = cv.fit_transform(corpus)

matrix = x.toarray()         

df = pd.DataFrame(matrix)
df.columns = cv.get_feature_names()
base_folders = os.listdir(args.foldername)
df.index = glob.glob('{}/*/article*'.format(args.foldername))
print(df)

# os.path.join(args.f, subfolder)
if args.svddims:
	TS = TruncatedSVD(args.svddims)
	P = TS.fit_transform(matrix)

print(P)

# def counts(text, m):
#     '''Args:
#             text
#                 list of tokenized words
#             m
#                 top m common words (?)
#         Output:
#             counts
#                 Counter object ('word', count)
#     '''
#     counts = {}
#     if args.base-vocab:
#         # counts = Counter(text).most_common(m)
#     else:
#         # counts = Counter(text)
#     return counts

# def create_vectors(docclass):
#     pass

    # pandas to numPy: cell 18 (np.array)
    # pd.datafram(counts)

'''Take a folder consisting of two subfolders
that contain documents of two different classes, the classes being the subfolder names,
and it will write an output file that consists of a table of document vectors.
'''


print("Loading data from directory {}.".format(args.foldername))

if not args.basedims:
    print("Using full vocabulary.")
else:
    print("Using only top {} terms by raw count.".format(args.basedims))

if args.tfidf:
    print("Applying tf-idf to raw counts.")

if args.svddims:
    print("Truncating matrix to {} dimensions via singular value decomposition.".format(args.svddims))

# THERE ARE SOME ERROR CONDITIONS YOU MAY HAVE TO HANDLE WITH CONTRADICTORY                                         
# PARAMETERS.                                                                                                       

print("Writing matrix to {}.".format(args.outputfile))
